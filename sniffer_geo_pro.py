# -*- coding: utf-8 -*-
import feedparser
import requests
import datetime
import re
import json
import os
import hashlib
import time
import csv
import signal
from collections import Counter
from urllib.parse import urljoin, urlparse
from bs4 import BeautifulSoup
import random

# ==================== RSSæºå‘ç°æ¨¡å—ï¼ˆä¼˜åŒ–ç‰ˆï¼‰ ====================

class RSSSourceFinder:
    def __init__(self, timeout=15):
        self.timeout = timeout
        # æ›´ä¸°å¯Œçš„ User-Agent åˆ—è¡¨ï¼Œéšæœºä½¿ç”¨ä»¥é™ä½è¢«å±è”½é£é™©
        self.user_agents = [
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.0 Safari/605.1.15",
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36 Edg/121.0.0.0",
            "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:120.0) Gecko/20100101 Firefox/120.0",
            "RSS-Finder-Plus/1.2 (+https://github.com/F-swanlight/)"
        ]
        self.session = self._create_session()
        
    def _create_session(self):
        """åˆ›å»ºå…·æœ‰éšæœºUser-Agentçš„ä¼šè¯"""
        session = requests.Session()
        session.headers.update({
            "User-Agent": random.choice(self.user_agents),
            "Accept": "application/rss+xml, application/atom+xml, text/xml, application/xml, */*",
            "Accept-Language": "en-US,en;q=0.9,zh-CN;q=0.8",
            "Accept-Encoding": "gzip, deflate, br"
        })
        return session
        
    def _rotate_user_agent(self):
        """è½®æ¢User-Agent"""
        self.session.headers.update({"User-Agent": random.choice(self.user_agents)})
    
    def fetch_json(self, url):
        try:
            self._rotate_user_agent()
            r = self.session.get(url, timeout=self.timeout)
            r.raise_for_status()
            return r.json()
        except Exception as e:
            print(f"[DEBUG] fetch_json error for {url}: {str(e)}")
            return None

    def fetch_resp(self, url, allow_redirects=True, method="GET", headers=None):
        try:
            self._rotate_user_agent()
            h = dict(self.session.headers)
            if headers:
                h.update(headers)
                
            if method == "HEAD":
                r = self.session.head(url, timeout=self.timeout, allow_redirects=allow_redirects, headers=h)
            else:
                # å¢åŠ é”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶
                max_retries = 2
                r = None
                for attempt in range(max_retries):
                    try:
                        r = self.session.get(url, timeout=self.timeout, allow_redirects=allow_redirects, headers=h)
                        r.raise_for_status()
                        return r
                    except requests.exceptions.RequestException as e:
                        if attempt < max_retries - 1:
                            sleep_time = 1 * (attempt + 1)
                            print(f"[DEBUG] Retry {attempt+1} for {url} after {sleep_time}s: {str(e)}")
                            time.sleep(sleep_time)
                        else:
                            raise e
            return r
        except Exception as e:
            print(f"[DEBUG] fetch_resp error for {url}: {str(e)}")
            return None

    def is_feed_response(self, resp):
        if resp is None:
            return False
            
        ctype = (resp.headers.get("Content-Type") or "").lower()
        
        # æ£€æŸ¥å¤´éƒ¨å†…å®¹ç±»å‹
        if any(t in ctype for t in ["application/rss+xml", "application/atom+xml", "application/xml", "text/xml"]):
            try:
                text_head = resp.text[:8192] if hasattr(resp, "text") else ""
                if "<rss" in text_head.lower() or "<feed" in text_head.lower() or "<channel" in text_head.lower():
                    return True
                    
                # æ£€æŸ¥æ˜¯å¦åŒ…å«å…¸å‹çš„RSS/Atomå…ƒç´ 
                if re.search(r'<(item|entry)>', text_head.lower()):
                    return True
            except:
                pass
                
        return False

    def normalize_url(self, u):
        if not u:
            return None
        if u.startswith("//"):
            u = "https:" + u
        if not urlparse(u).scheme:
            u = "https://" + u
        return u

    def get_homepages_from_openalex(self, issn):
        url = f"https://api.openalex.org/sources/ISSN:{issn}"
        data = self.fetch_json(url)
        homes = []
        if data and isinstance(data, dict):
            homepage = self.normalize_url(data.get("homepage_url"))
            if homepage:
                homes.append(homepage)
            for a in data.get("alternate_urls") or []:
                a = self.normalize_url(a)
                if a:
                    homes.append(a)
                    
        # å¦‚æœOpenAlexæ— è¿”å›ï¼Œå°è¯•å…¶ä»–æ–¹å¼
        if not homes and issn:
            try:
                issn_no_dash = issn.replace("-", "")
                homes.append(f"https://www.doi.org/{issn}")
                homes.append(f"https://www.doi.org/{issn_no_dash}")
                homes.extend([
                    f"https://www.sciencedirect.com/journal/{issn}",
                    f"https://onlinelibrary.wiley.com/journal/{issn}",
                    f"https://www.tandfonline.com/journals/{issn}",
                    f"https://journals.sagepub.com/{issn}"
                ])
            except:
                pass
                
        return list(dict.fromkeys(homes))

    def extract_feed_links_from_html(self, url, html):
        try:
            soup = BeautifulSoup(html, "html.parser")
            feed_urls = set()
            
            for link in soup.find_all("link", attrs={"rel": True, "href": True}):
                rels = " ".join([r.lower() for r in link.get("rel") or []])
                typ = (link.get("type") or "").lower()
                if "alternate" in rels and any(t in typ for t in ["rss+xml", "atom+xml", "xml", "rss"]):
                    href = link.get("href")
                    if href:
                        feed_urls.add(urljoin(url, href))
            
            for a in soup.find_all("a", href=True):
                href = a.get("href") or ""
                text = (a.get_text() or "").lower()
                if any(k in href.lower() for k in ["rss", "feed", "atom", "xml", "syndication"]) or \
                   any(k in text.lower() for k in ["rss", "feed", "atom", "xml", "syndication"]):
                    feed_urls.add(urljoin(url, href))
                    
            for meta in soup.find_all("meta"):
                if meta.get("name", "").lower() == "description" and "rss" in meta.get("content", "").lower():
                    urls = re.findall(r'https?://\S+', meta.get("content", ""))
                    for u in urls:
                        if any(k in u.lower() for k in ["rss", "feed", "atom", "xml"]):
                            feed_urls.add(u)
                            
            return list(feed_urls)
        except Exception as e:
            print(f"[DEBUG] extract_feed_links_from_html error: {str(e)}")
            return []

    def discover_official_feeds(self, home_url):
        out = []
        home_url = self.normalize_url(home_url)
        if not home_url:
            return out
            
        print(f"[DEBUG] Checking homepage: {home_url}")

        try:
            resp = self.fetch_resp(home_url)
            if resp is not None:
                for fu in self.extract_feed_links_from_html(home_url, resp.text):
                    print(f"[DEBUG] Testing potential feed: {fu}")
                    r = self.fetch_resp(fu)
                    if self.is_feed_response(r):
                        out.append(fu)
                        print(f"[DEBUG] Found valid feed: {fu}")
        except Exception as e:
            print(f"[DEBUG] Error checking homepage: {str(e)}")

        parsed = urlparse(home_url)
        base = f"{parsed.scheme}://{parsed.netloc}"
        bases = [home_url.rstrip("/"), base.rstrip("/")]
        
        suffixes = [
            "rss", "feed", "rss.xml", "atom.xml", "feeds", "index.xml",
            "feed/rss", "rss/feed", "rss/all", "feeds/posts/default",
            "feed.xml", "atom", "syndication", "rss/index", "news/feed",
            "current.rss", "current.xml", "current-issue", "current-issue/feed",
            "current-issue/rss", "latest/rss", "latest.xml"
        ]
        
        for b in bases:
            for suf in suffixes:
                fu = f"{b}/{suf}"
                print(f"[DEBUG] Testing common pattern: {fu}")
                r = self.fetch_resp(fu)
                if self.is_feed_response(r):
                    out.append(fu)
                    print(f"[DEBUG] Found valid feed: {fu}")

        return list(dict.fromkeys(out))

    def try_publisher_specific_feeds(self, journal_title, issn):
        """å°è¯•å„å¤§å‡ºç‰ˆç¤¾ç‰¹å®šçš„RSSæºæ ¼å¼"""
        feeds = []
        
        clean_title = re.sub(r'[^\w\s]', '', journal_title.lower())
        slug = "-".join(clean_title.split())
        
        # 1. Elsevier ScienceDirect
        if issn:
            for val in [issn, issn.replace("-", "")]:
                urls = [
                    f"https://rss.sciencedirect.com/publication/science/{val}",
                    f"https://www.sciencedirect.com/journal/{val}/latest-articles/rss"
                ]
                for url in urls:
                    r = self.fetch_resp(url)
                    if self.is_feed_response(r):
                        feeds.append(url)
        
        # 2. Wiley
        if issn:
            urls = [
                f"https://onlinelibrary.wiley.com/feed/{issn}/most-recent",
                f"https://onlinelibrary.wiley.com/action/showFeed?type=etoc&feed=rss&jc={issn}"
            ]
            for url in urls:
                r = self.fetch_resp(url)
                if self.is_feed_response(r):
                    feeds.append(url)
        
        # 3. Nature
        if slug:
            nature_url = f"https://www.nature.com/{slug}.rss"
            r = self.fetch_resp(nature_url)
            if self.is_feed_response(r):
                feeds.append(nature_url)
            
        # 4. MDPI
        if slug:
            mdpi_url = f"https://www.mdpi.com/rss/journal/{slug}"
            r = self.fetch_resp(mdpi_url)
            if self.is_feed_response(r):
                feeds.append(mdpi_url)
            
        # 5. SpringerLink
        if issn:
            springer_url = f"https://link.springer.com/journal/{issn}.rss"
            r = self.fetch_resp(springer_url)
            if self.is_feed_response(r):
                feeds.append(springer_url)
                
        # 6. Taylor & Francis
        if slug:
            tf_url = f"https://www.tandfonline.com/feed/rss/{slug}"
            r = self.fetch_resp(tf_url)
            if self.is_feed_response(r):
                feeds.append(tf_url)
                
        # 7. SAGE Journals
        if slug:
            sage_url = f"https://journals.sagepub.com/action/showFeed?ui=0&mi=ehikzz&ai=2b4&jc={slug}&type=etoc&feed=rss"
            r = self.fetch_resp(sage_url)
            if self.is_feed_response(r):
                feeds.append(sage_url)
                
        return feeds

    def find_rss_for_journal(self, title, issn):
        """ä¸ºå•ä¸ªæœŸåˆŠæŸ¥æ‰¾RSSæºï¼ˆä¼˜åŒ–ç‰ˆï¼‰"""
        try:
            print(f"\n[DEBUG] Finding RSS for: {title} (ISSN: {issn})")
            
            journal_timeout = 60  # ç§’
            start_time = time.time()
            
            publisher_feeds = self.try_publisher_specific_feeds(title, issn)
            if publisher_feeds:
                print(f"[DEBUG] Found publisher-specific feed: {publisher_feeds[0]}")
                return publisher_feeds[0], "publisher_specific"
            
            if time.time() - start_time > journal_timeout:
                print(f"[WARN] æœŸåˆŠå¤„ç†è¶…æ—¶: {title}")
                return None, "timeout"
            
            homes = self.get_homepages_from_openalex(issn)
            
            for home in homes:
                if time.time() - start_time > journal_timeout:
                    print(f"[WARN] æœŸåˆŠå¤„ç†è¶…æ—¶: {title}")
                    return None, "timeout"
                feeds = self.discover_official_feeds(home)
                if feeds:
                    print(f"[DEBUG] Found feed from homepage: {feeds[0]}")
                    return feeds[0], "official"
            
            try:
                if time.time() - start_time > journal_timeout:
                    print(f"[WARN] æœŸåˆŠå¤„ç†è¶…æ—¶: {title}")
                    return None, "timeout"
                search_term = f"{title} journal rss feed"
                search_url = f"https://www.bing.com/search?q={search_term}"
                resp = self.fetch_resp(search_url)
                
                if resp and resp.text:
                    soup = BeautifulSoup(resp.text, "html.parser")
                    links = soup.find_all("a", href=True)
                    for link in links:
                        href = link.get("href", "")
                        if any(term in href.lower() for term in ["rss", "feed", "atom", ".xml"]):
                            r = self.fetch_resp(href)
                            if self.is_feed_response(r):
                                print(f"[DEBUG] Found feed from search: {href}")
                                return href, "search"
            except Exception as e:
                print(f"[DEBUG] Search engine method failed: {str(e)}")
            
            return None, None
        except Exception as e:
            print(f"[ERROR] Journal processing error: {str(e)}")
            return None, "error"

    def update_journal_rss_sources(self, journal_csv_file, output_file="journals_with_rss.csv"):
        """æ‰¹é‡æ›´æ–°æœŸåˆŠRSSæº"""
        print(f"[INFO] ğŸ” å¼€å§‹æŸ¥æ‰¾æœŸåˆŠRSSæº...")
        
        def timeout_handler(signum, frame):
            raise TimeoutError("RSSå¤„ç†è¶…æ—¶")
        
        signal.signal(signal.SIGALRM, timeout_handler)
        signal.alarm(1800)  # 30åˆ†é’Ÿ
        
        rows = []
        results = []
        
        try:
            if not os.path.exists(journal_csv_file):
                print(f"[ERROR] æœŸåˆŠæ–‡ä»¶ä¸å­˜åœ¨: {journal_csv_file}")
                return []
            if not os.access(journal_csv_file, os.R_OK):
                print(f"[ERROR] æœŸåˆŠæ–‡ä»¶æ— æ³•è¯»å–(æƒé™é—®é¢˜): {journal_csv_file}")
                return []
            try:
                with open(journal_csv_file, "r", encoding="utf-8-sig", newline="") as f:
                    reader = csv.DictReader(f)
                    rows = list(reader)
            except UnicodeDecodeError:
                print(f"[ERROR] æ–‡ä»¶ç¼–ç é—®é¢˜ï¼Œå°è¯•ä¸åŒç¼–ç ...")
                with open(journal_csv_file, "r", encoding="latin-1", newline="") as f:
                    reader = csv.DictReader(f)
                    rows = list(reader)
        except Exception as e:
            print(f"[ERROR] è¯»å–æœŸåˆŠæ–‡ä»¶å¤±è´¥: {str(e)}")
            return []
        
        total = len(rows)
        found_count = 0
        
        try:
            for i, row in enumerate(rows, 1):
                title = row.get("title", "").strip()
                issn = row.get("issn", "").strip()
                zone = row.get("zone", "").strip()
                
                print(f"[INFO] ğŸ“– å¤„ç†è¿›åº¦: {i}/{total} - {title[:50]}...")
                
                rss_url, rss_source = self.find_rss_for_journal(title, issn)
                
                result = {
                    "index": row.get("index", i),
                    "title": title,
                    "issn": issn,
                    "zone": zone,
                    "rss_url": rss_url or "",
                    "rss_source": rss_source or ""
                }
                results.append(result)
                
                if rss_url:
                    found_count += 1
                    print(f"[SUCCESS] âœ… æ‰¾åˆ°RSSæº: {rss_source}")
                else:
                    print(f"[WARN] âŒ æœªæ‰¾åˆ°RSSæº")
                
                # é€Ÿç‡æ§åˆ¶
                if i % 10 == 0:  # æ¯10ä¸ªè¾ƒé•¿ç­‰å¾…
                    wait_time = random.uniform(5, 10)
                    print(f"[INFO] è¾ƒé•¿ç­‰å¾… {wait_time:.1f} ç§’...")
                    time.sleep(wait_time)
                elif i % 3 == 0:
                    wait_time = random.uniform(2, 5)
                    print(f"[INFO] ç­‰å¾… {wait_time:.1f} ç§’...")
                    time.sleep(wait_time)
                
                # å®šæœŸä¿å­˜ä¸´æ—¶ç»“æœ
                if i % 20 == 0:
                    try:
                        temp_file = output_file + ".temp"
                        fieldnames = ["index", "title", "issn", "zone", "rss_url", "rss_source"]
                        with open(temp_file, "w", encoding="utf-8", newline="") as f:
                            writer = csv.DictWriter(f, fieldnames=fieldnames)
                            writer.writeheader()
                            writer.writerows(results)
                        print(f"[INFO] ğŸ’¾ ä¸´æ—¶ç»“æœå·²ä¿å­˜è‡³: {temp_file}")
                    except Exception as e:
                        print(f"[WARN] ä¿å­˜ä¸´æ—¶ç»“æœå¤±è´¥: {str(e)}")
        except TimeoutError:
            print("[ERROR] RSSæºæŸ¥æ‰¾å¤„ç†è¶…æ—¶ï¼Œè¿”å›å·²å¤„ç†çš„ç»“æœ")
        finally:
            signal.alarm(0)
        
        try:
            fieldnames = ["index", "title", "issn", "zone", "rss_url", "rss_source"]
            with open(output_file, "w", encoding="utf-8", newline="") as f:
                writer = csv.DictWriter(f, fieldnames=fieldnames)
                writer.writeheader()
                writer.writerows(results)
            print(f"[INFO] ğŸ“Š RSSæºæŸ¥æ‰¾å®Œæˆ: {found_count}/{total} ä¸ªæœŸåˆŠæ‰¾åˆ°RSSæº")
            print(f"[INFO] ğŸ’¾ ç»“æœå·²ä¿å­˜è‡³: {output_file}")
        except Exception as e:
            print(f"[ERROR] ä¿å­˜RSSç»“æœå¤±è´¥: {str(e)}")
        
        return [r for r in results if r["rss_url"]]

# ==================== ä¸»æ¨é€ç³»ç»Ÿ ====================

CORE_KEYWORDS = [
    "ç¢³é…¸ç›å²©", "carbonate", "carbonate rock", "limestone", "ç°å²©", "ç™½äº‘å²©", "dolomite", "dolomitic",
    "å¾®ç”Ÿç‰©çŸ¿åŒ–", "microbialite", "microbial mineralization", "biomineralization", "microbial carbonate",
    "å¤©ç„¶æ°¢", "natural hydrogen", "ç™½æ°¢", "white hydrogen", "native hydrogen", "geological hydrogen",
    "å¤§æ´‹æ°§åŒ–", "ocean oxidation", "ocean redox", "oceanic oxidation", "marine oxidation", "redox evolution"
]

AUXILIARY_KEYWORDS = [
    "ååº”ç½‘ç»œ", "reaction network", "reacnetgenerator", "åˆ†å­åŠ¨åŠ›å­¦", "molecular dynamics", "MD simulation",
    "æœºå™¨å­¦ä¹ ", "machine learning", "AI", "artificial intelligence", "ç”Ÿæˆå¼AI", "generative AI",
    "æ•°æ®æŒ–æ˜", "data mining", "æ·±åº¦å­¦ä¹ ", "deep learning", "ç¥ç»ç½‘ç»œ", "neural network",
    "åœ°çƒåŒ–å­¦", "geochemistry", "çŸ¿åŒ–", "mineralization", "æ²‰ç§¯", "sedimentary",
    "å¤ç¯å¢ƒ", "paleoenvironment", "æˆå²©", "diagenesis", "é»„é“çŸ¿", "pyrite",
    "æ°§åŒ–", "oxidation", "æ°§", "oxygen", "æµ·æ´‹", "marine", "deep sea",
    "simulation", "modeling", "computational", "numerical", "fold", "folding",
    "æ„é€ ", "structure", "tectonics", "åœ°å±‚", "stratigraphy", "deformation"
]

ZONE_WEIGHTS = {
    "1åŒº": 50,
    "2åŒº": 30,
    "3åŒº": 20,
    "4åŒº": 10,
    "": 15


WECHAT_WEBHOOK = "https://qyapi.weixin.qq.com/cgi-bin/webhook/send?key=d'saho'ifvhaDVBAVNSOVSNAP"

EXCLUDED_KEYWORDS = [
    "carbonate", "limestone", "dolomite", "microbial", "hydrogen", "oxidation", "ocean",
    "ç¢³é…¸ç›", "ç°å²©", "ç™½äº‘å²©", "å¾®ç”Ÿç‰©", "æ°¢", "æ°§åŒ–", "æµ·æ´‹", "çŸ¿åŒ–"
]

HISTORY_FILE = "pushed_articles.json"
PUSH_SCHEDULE_FILE = "push_schedule.json"
RSS_STATUS_FILE = "rss_status.json"
JOURNAL_RSS_FILE = "journals_with_rss.csv"
JOURNAL_LIST_FILE = "journals_1-260.csv"
HISTORY_DAYS = 60
DUPLICATE_CHECK_DAYS = 7
MAX_PUSH_PER_BATCH = 6

TRANSLATE_API_URL = "https://api.mymemory.translated.net/get"

# æ¯å‘¨æ›´æ–°RSSæºçš„æ˜ŸæœŸé…ç½®ï¼šPythonä¸­å‘¨ä¸€=0ï¼Œå‘¨æ—¥=6
WEEKLY_RSS_UPDATE_DAY = 6  # å‘¨æ—¥

def load_rss_feeds_from_csv(csv_file):
    feeds = []
    try:
        with open(csv_file, "r", encoding="utf-8-sig", newline="") as f:
            reader = csv.DictReader(f)
            for row in reader:
                rss_url = row.get("rss_url", "").strip()
                if rss_url:
                    feeds.append({
                        "url": rss_url,
                        "title": row.get("title", ""),
                        "source": row.get("rss_source", ""),
                        "zone": row.get("zone", ""),
                        "issn": row.get("issn", "")
                    })
    except FileNotFoundError:
        print(f"[WARN] RSSæºæ–‡ä»¶ä¸å­˜åœ¨: {csv_file}")
    return feeds

def translate_to_chinese(text):
    try:
        if any('\u4e00' <= char <= '\u9fff' for char in text):
            return text
        if len(text) > 200:
            text = text[:200] + "..."
        params = {'q': text, 'langpair': 'en|zh-CN'}
        response = requests.get(TRANSLATE_API_URL, params=params, timeout=5)
        response.raise_for_status()
        result = response.json()
        if result.get('responseStatus') == 200:
            translated = result.get('responseData', {}).get('translatedText', '')
            if translated and translated != text:
                return translated
        return text
    except Exception as e:
        print(f"[WARN] ç¿»è¯‘å¤±è´¥: {e}")
        return text

def load_rss_status():
    if os.path.exists(RSS_STATUS_FILE):
        try:
            with open(RSS_STATUS_FILE, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            print(f"[ERROR] è¯»å–RSSçŠ¶æ€å¤±è´¥: {e}")
            return {}
    return {}

def save_rss_status(status):
    try:
        with open(RSS_STATUS_FILE, 'w', encoding='utf-8') as f:
            json.dump(status, f, ensure_ascii=False, indent=2)
    except Exception as e:
        print(f"[ERROR] ä¿å­˜RSSçŠ¶æ€å¤±è´¥: {e}")

def load_pushed_articles():
    if os.path.exists(HISTORY_FILE):
        try:
            with open(HISTORY_FILE, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            print(f"[ERROR] è¯»å–å†å²è®°å½•å¤±è´¥: {e}")
            return {}
    return {}

def save_pushed_articles(pushed_articles):
    try:
        with open(HISTORY_FILE, 'w', encoding='utf-8') as f:
            json.dump(pushed_articles, f, ensure_ascii=False, indent=2)
        print(f"[INFO] å†å²è®°å½•å·²ä¿å­˜")
    except Exception as e:
        print(f"[ERROR] ä¿å­˜å†å²è®°å½•å¤±è´¥: {e}")

def load_push_schedule():
    if os.path.exists(PUSH_SCHEDULE_FILE):
        try:
            with open(PUSH_SCHEDULE_FILE, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            print(f"[ERROR] è¯»å–æ¨é€è®¡åˆ’å¤±è´¥: {e}")
            return {}
    return {}

def save_push_schedule(schedule):
    try:
        with open(PUSH_SCHEDULE_FILE, 'w', encoding='utf-8') as f:
            json.dump(schedule, f, ensure_ascii=False, indent=2)
        print(f"[INFO] æ¨é€è®¡åˆ’å·²ä¿å­˜")
    except Exception as e:
        print(f"[ERROR] ä¿å­˜æ¨é€è®¡åˆ’å¤±è´¥: {e}")

def clean_old_records(pushed_articles, days=HISTORY_DAYS):
    cutoff_date = datetime.datetime.now() - datetime.timedelta(days=days)
    cutoff_str = cutoff_date.strftime("%Y-%m-%d")
    keys_to_remove = [k for k in pushed_articles if k < cutoff_str]
    for key in keys_to_remove:
        del pushed_articles[key]
    if keys_to_remove:
        print(f"[INFO] æ¸…ç†äº† {len(keys_to_remove)} å¤©çš„æ—§è®°å½•")

def generate_article_hash(title, link):
    title = (title or "").strip()
    link = (link or "").strip()
    content = f"{title}||{link}"
    return hashlib.md5(content.encode('utf-8')).hexdigest()

def is_article_duplicate(article_hash, pushed_articles, today):
    current_date = datetime.datetime.strptime(today, "%Y-%m-%d")
    for i in range(DUPLICATE_CHECK_DAYS):
        check_date = (current_date - datetime.timedelta(days=i)).strftime("%Y-%m-%d")
        if check_date in pushed_articles and article_hash in pushed_articles[check_date]:
            return True
    return False

def extract_publication_date(entry):
    for date_field in ['published', 'updated', 'pubDate', 'date']:
        if date_field in entry and entry[date_field]:
            try:
                date_str = entry[date_field]
                formats = [
                    '%a, %d %b %Y %H:%M:%S %z',
                    '%a, %d %b %Y %H:%M:%S %Z',
                    '%Y-%m-%dT%H:%M:%S%z',
                    '%Y-%m-%dT%H:%M:%SZ',
                    '%Y-%m-%d %H:%M:%S',
                    '%Y-%m-%d',
                ]
                for fmt in formats:
                    try:
                        dt = datetime.datetime.strptime(date_str, fmt)
                        return dt.strftime('%Y-%m-%d')
                    except:
                        continue
                date_match = re.search(r'(\d{4}[-/]\d{1,2}[-/]\d{1,2})', date_str)
                if date_match:
                    return date_match.group(1).replace('/', '-')
            except:
                pass
    return None

def calculate_priority_score(text, zone=""):
    text_lower = text.lower()
    core_matches = sum(1 for k in CORE_KEYWORDS if k.lower() in text_lower)
    aux_matches = sum(1 for k in AUXILIARY_KEYWORDS if k.lower() in text_lower)
    keyword_score = core_matches * 10 + aux_matches * 1
    zone_weight = ZONE_WEIGHTS.get(zone, ZONE_WEIGHTS[""])
    priority_score = keyword_score + zone_weight
    return priority_score, core_matches, aux_matches, zone_weight

def extract_meaningful_phrases(text):
    text = re.sub(r'[^\w\s\u4e00-\u9fff-]', ' ', text)
    text = re.sub(r'\s+', ' ', text).strip()
    phrases = []
    english_phrases = re.findall(r'\b[A-Za-z][\w-]*(?:\s+[A-Za-z][\w-]*){1,3}\b', text)
    for phrase in english_phrases:
        phrase = phrase.strip().lower()
        words = phrase.split()
        if 2 <= len(words) <= 4 and 6 <= len(phrase) <= 40:
            if not all(w.isdigit() for w in words) and not all(len(w) <= 2 for w in words):
                should_exclude = any(ex.lower() in phrase for ex in EXCLUDED_KEYWORDS)
                stop_phrases = ["in the", "of the", "and the", "for the", "this is", "there are"]
                if any(phrase.startswith(stop) for stop in stop_phrases):
                    should_exclude = True
                if not should_exclude:
                    phrases.append(phrase)
    chinese_phrases = re.findall(r'[\u4e00-\u9fff]{2,8}', text)
    for phrase in chinese_phrases:
        if 2 <= len(phrase) <= 8:
            if not any(ex in phrase for ex in EXCLUDED_KEYWORDS):
                phrases.append(phrase)
    return phrases

def has_core_keywords(text):
    text_lower = text.lower()
    return any(keyword.lower() in text_lower for keyword in CORE_KEYWORDS)

def filter_articles(feed_info, today, pushed_articles, rss_status):
    feed_url = feed_info["url"]
    feed_title = feed_info.get("title", "")
    feed_zone = feed_info.get("zone", "")
    
    headers = {
        'User-Agent': "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        'Accept': 'application/rss+xml, application/xml, text/xml, */*',
        'Accept-Language': 'en-US,en;q=0.9,zh-CN;q=0.8',
        'Accept-Encoding': 'gzip, deflate, br',
        'Connection': 'keep-alive',
    }
    
    try:
        zone_display = f"[{feed_zone}]" if feed_zone else ""
        print(f"[INFO] ğŸ” æ­£åœ¨è¯»å–RSS: {feed_title[:30]}...{zone_display} ({feed_info.get('source', 'unknown')})")
        
        max_retries = 3
        resp = None
        for attempt in range(max_retries):
            try:
                resp = requests.get(feed_url, timeout=15, headers=headers, allow_redirects=True)
                resp.raise_for_status()
                break
            except requests.exceptions.RequestException as e:
                if attempt < max_retries - 1:
                    print(f"[WARN] ç¬¬{attempt+1}æ¬¡å°è¯•å¤±è´¥ï¼Œç­‰å¾…é‡è¯•: {e}")
                    time.sleep(2)
                    continue
                else:
                    raise e
        
        rss_status[feed_url] = {
            'last_success': today,
            'status': 'success',
            'error': None,
            'journal': feed_title,
            'zone': feed_zone
        }
        
        feed = feedparser.parse(resp.content)
        if not hasattr(feed, 'entries') or len(feed.entries) == 0:
            print(f"[WARN] RSSæºè¿”å›ç©ºå†…å®¹: {feed_title}")
            rss_status[feed_url]['status'] = 'empty'
            return [], []
        
        filtered_articles = []
        all_meaningful_phrases = []
        duplicate_count = 0
        total_articles = len(feed.entries)
        
        for entry in feed.entries:
            title = entry.title or ""
            link = entry.link or ""
            summary = entry.get("summary", "") or entry.get("description", "") or ""
            pub_date = extract_publication_date(entry)
            article_hash = generate_article_hash(title, link)
            if is_article_duplicate(article_hash, pushed_articles, today):
                duplicate_count += 1
                continue
            text = title + " " + summary
            meaningful_phrases = extract_meaningful_phrases(text)
            all_meaningful_phrases.extend(meaningful_phrases)
            if has_core_keywords(text):
                priority_score, core_matches, aux_matches, zone_weight = calculate_priority_score(text, feed_zone)
                chinese_title = translate_to_chinese(title)
                article_info = {
                    'title': title,
                    'chinese_title': chinese_title,
                    'link': link,
                    'hash': article_hash,
                    'priority_score': priority_score,
                    'core_matches': core_matches,
                    'aux_matches': aux_matches,
                    'zone': feed_zone,
                    'zone_weight': zone_weight,
                    'source': feed_title,
                    'source_type': feed_info.get('source', 'unknown'),
                    'text': text,
                    'pub_date': pub_date or "æœªçŸ¥æ—¥æœŸ"
                }
                filtered_articles.append(article_info)
        
        print(f"[INFO] âœ… å…±{total_articles}ç¯‡ï¼Œç­›é€‰{len(filtered_articles)}æ¡æ ¸å¿ƒåŒ¹é…ï¼Œè·³è¿‡{duplicate_count}æ¡é‡å¤")
        return filtered_articles, all_meaningful_phrases
        
    except requests.exceptions.Timeout:
        error_msg = "â° è¯·æ±‚è¶…æ—¶"
        print(f"[ERROR] {feed_title} {error_msg}")
        rss_status[feed_url] = {'last_attempt': today, 'status': 'timeout', 'error': error_msg, 'journal': feed_title, 'zone': feed_zone}
        return [], []
    except requests.exceptions.HTTPError as e:
        error_msg = f"HTTPé”™è¯¯: {e.response.status_code}"
        print(f"[ERROR] {feed_title} {error_msg}")
        rss_status[feed_url] = {'last_attempt': today, 'status': 'http_error', 'error': error_msg, 'journal': feed_title, 'zone': feed_zone}
        return [], []
    except requests.exceptions.ConnectionError:
        error_msg = "è¿æ¥é”™è¯¯"
        print(f"[ERROR] {feed_title} {error_msg}")
        rss_status[feed_url] = {'last_attempt': today, 'status': 'connection_error', 'error': error_msg, 'journal': feed_title, 'zone': feed_zone}
        return [], []
    except Exception as e:
        error_msg = f"æœªçŸ¥é”™è¯¯: {str(e)}"
        print(f"[ERROR] {feed_title} {error_msg}")
        rss_status[feed_url] = {'last_attempt': today, 'status': 'unknown_error', 'error': error_msg, 'journal': feed_title, 'zone': feed_zone}
        return [], []

def format_article_for_push(article, index):
    source_name = article.get('source', 'Unknown')
    zone = article.get('zone', '')
    zone_display = f" [{zone}]" if zone else ""
    pub_date = article.get('pub_date', 'æœªçŸ¥æ—¥æœŸ')
    display_title = article['chinese_title'] if article['chinese_title'] != article['title'] else article['title']
    result = f"ğŸ“„ {index}. {display_title}"
    if article['chinese_title'] != article['title']:
        result += f"\nğŸ”¤ åŸæ ‡é¢˜: {article['title']}"
    result += f"\nğŸ›ï¸ æ¥æº: {source_name}{zone_display}\nğŸ“… æ—¥æœŸ: {pub_date}\nğŸ”— é“¾æ¥: {article['link']}"
    return result

def find_historical_articles(pushed_articles, push_schedule, today, needed_count):
    if needed_count <= 0:
        return []
    print(f"[INFO] ğŸ” æŸ¥æ‰¾å†å²æœªæ¨é€æ–‡ç« ï¼Œéœ€è¦è¡¥å…… {needed_count} ç¯‡")
    pushed_hashes = set()
    for date_key in pushed_articles:
        for article_hash in pushed_articles[date_key]:
            pushed_hashes.add(article_hash)
    scheduled_hashes = set()
    if today in push_schedule:
        for article in push_schedule[today]:
            scheduled_hashes.add(article['hash'])
    all_historical_articles = []
    history_schedule_files = []
    for i in range(1, 11):
        past_date = (datetime.datetime.now() - datetime.timedelta(days=i)).strftime("%Y-%m-%d")
        filename = f"push_schedule_{past_date}.json"
        if os.path.exists(filename):
            history_schedule_files.append((past_date, filename))
    for date_str, filename in history_schedule_files:
        try:
            with open(filename, 'r', encoding='utf-8') as f:
                history_data = json.load(f)
                if date_str in history_data:
                    for article in history_data[date_str]:
                        if article['hash'] not in pushed_hashes and article['hash'] not in scheduled_hashes:
                            all_historical_articles.append(article)
        except Exception as e:
            print(f"[WARN] è¯»å–å†å²æ¨é€è®¡åˆ’å¤±è´¥ {filename}: {e}")
    if len(all_historical_articles) < needed_count:
        for date_key in sorted(push_schedule.keys()):
            if date_key < today:
                for article in push_schedule[date_key]:
                    if article['hash'] not in pushed_hashes and article['hash'] not in scheduled_hashes:
                        all_historical_articles.append(article)
    all_historical_articles = sorted(all_historical_articles, key=lambda x: x['priority_score'], reverse=True)
    return all_historical_articles[:needed_count]

def get_top_meaningful_phrases(all_phrases, top_n=5):
    if not all_phrases:
        return []
    normalized_phrases = [p.strip() for p in all_phrases if p and len(p.strip()) >= 4]
    if not normalized_phrases:
        return []
    phrase_count = Counter(normalized_phrases)
    return phrase_count.most_common(top_n)

def push_to_wechat(text):
    print(f"[INFO] ğŸ“¤ å‡†å¤‡æ¨é€å†…å®¹ï¼Œé•¿åº¦ä¸º {len(text)} å­—ç¬¦")
    maxlen = 1800
    for i in range(0, len(text), maxlen):
        chunk = text[i:i+maxlen]
        try:
            response = requests.post(WECHAT_WEBHOOK, json={"msgtype": "text", "text": {"content": chunk}})
            print(f"[INFO] å¾®ä¿¡æ¨é€å“åº”: {response.text}")
            if response.status_code != 200:
                print(f"[ERROR] æ¨é€å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
        except Exception as e:
            print(f"[ERROR] æ¨é€åˆ°å¾®ä¿¡æ—¶å‡ºé”™: {e}")

def get_rss_status_summary(rss_status, total_feeds):
    success = len([s for s in rss_status.values() if s.get('status') == 'success'])
    failed = total_feeds - success
    zone_stats = {}
    for status in rss_status.values():
        if status.get('status') == 'success':
            zone = status.get('zone', 'æœªçŸ¥')
            zone_stats[zone] = zone_stats.get(zone, 0) + 1
    return {
        'total': total_feeds,
        'success': success,
        'failed': failed,
        'success_rate': round((success / total_feeds * 100) if total_feeds > 0 else 0, 1),
        'zone_stats': zone_stats
    }

def main():
    today = datetime.datetime.now().strftime("%Y-%m-%d")
    current_time = datetime.datetime.now().strftime("%H:%M:%S")
    
    print(f"\n[INFO] ğŸ”ï¸ æŠ˜å åœ°å±‚æ¨é€ç³»ç»Ÿå¯åŠ¨ [åˆ†åŒºè¯„åˆ†ä¼˜åŒ–ç‰ˆ v2.0]")
    print(f"[INFO] ğŸ“… æ—¥æœŸ: {today} â° æ—¶é—´: {current_time}")
    print(f"[INFO] ğŸ‘¤ ç”¨æˆ·: F-swanlight")
    print(f"[INFO] ğŸ¯ æ¯æ‰¹æ¬¡æœ€å¤šæ¨é€: {MAX_PUSH_PER_BATCH} ç¯‡")
    print(f"[INFO] ğŸ¯ åˆ†åŒºæƒé‡: 1åŒº(+50) 2åŒº(+30) 3åŒº(+20) 4åŒº(+10)")
    
    print(f"\n[INFO] ğŸ”„ ç¬¬ä¸€æ­¥ï¼šæ›´æ–°æœŸåˆŠRSSæºï¼ˆä»…åœ¨æ¯å‘¨æ—¥æ‰§è¡Œï¼‰...")
    rss_finder = RSSSourceFinder(timeout=12)

    # æ˜¯å¦å¼ºåˆ¶æ›´æ–°ï¼ˆç¯å¢ƒå˜é‡FORCE_RSS_UPDATE=1 å¯ä¸´æ—¶è¦†ç›–å‘¨æ—¥é™åˆ¶ï¼‰
    force_update_flag = os.getenv("FORCE_RSS_UPDATE", "").strip() == "1"
    is_sunday = datetime.datetime.now().weekday() == WEEKLY_RSS_UPDATE_DAY
    
    should_update_rss = False
    if force_update_flag:
        print("[INFO] âš ï¸ ç¯å¢ƒå˜é‡ FORCE_RSS_UPDATE=1 å·²è®¾ç½®ï¼Œæœ¬æ¬¡å°†å¼ºåˆ¶æ›´æ–°RSSæºï¼ˆå¿½ç•¥å‘¨æ—¥é™åˆ¶ï¼‰")
        should_update_rss = True
    else:
        if is_sunday:
            # å‘¨æ—¥æ‰§è¡Œï¼šè‹¥ä»Šæ—¥å°šæœªæ›´æ–°åˆ™æ‰§è¡Œ
            if os.path.exists(JOURNAL_RSS_FILE):
                mtime = os.path.getmtime(JOURNAL_RSS_FILE)
                mdate = datetime.datetime.fromtimestamp(mtime).strftime("%Y-%m-%d")
                if mdate == today:
                    should_update_rss = False
                    print(f"[INFO] âœ… ä»Šæ—¥ï¼ˆå‘¨æ—¥ï¼‰RSSæºæ–‡ä»¶å·²æ›´æ–°ï¼Œè·³è¿‡é‡å¤æ›´æ–°")
                else:
                    should_update_rss = True
                    print(f"[INFO] ğŸ“… å‘¨æ—¥ä¸”ä»Šæ—¥æœªæ›´æ–°ï¼Œå°†æ‰§è¡ŒRSSæºå‘ç°")
            else:
                should_update_rss = True
                print(f"[INFO] ğŸ“… å‘¨æ—¥ä¸”ä¸å­˜åœ¨RSSæºæ–‡ä»¶ï¼Œå°†æ‰§è¡Œé¦–æ¬¡ç”Ÿæˆ")
        else:
            # éå‘¨æ—¥é»˜è®¤è·³è¿‡ï¼›è‹¥æ–‡ä»¶ä¸å­˜åœ¨åˆ™é¦–æ¬¡ç”Ÿæˆ
            if os.path.exists(JOURNAL_RSS_FILE):
                should_update_rss = False
                print(f"[INFO] ğŸ“† éå‘¨æ—¥ï¼Œè·³è¿‡RSSæºå‘ç°ï¼ˆä½¿ç”¨ç°æœ‰æ–‡ä»¶ï¼‰")
            else:
                should_update_rss = True
                print(f"[WARN] ğŸ“ æœªå‘ç°RSSæºæ–‡ä»¶ï¼Œå°½ç®¡éå‘¨æ—¥ï¼Œä»å°†æ‰§è¡Œé¦–æ¬¡ç”Ÿæˆä»¥ä¿è¯å¯ç”¨")

    SKIP_RSS_UPDATE = False  # æµ‹è¯•æ—¶è®¾ä¸º True
    if SKIP_RSS_UPDATE:
        should_update_rss = False
        print("[INFO] âš ï¸ æµ‹è¯•æ¨¡å¼ï¼šè·³è¿‡RSSæºæ›´æ–°")
    
    if should_update_rss:
        if os.path.exists(JOURNAL_LIST_FILE):
            _ = rss_finder.update_journal_rss_sources(JOURNAL_LIST_FILE, JOURNAL_RSS_FILE)
        else:
            print(f"[WARN] æœŸåˆŠåˆ—è¡¨æ–‡ä»¶ä¸å­˜åœ¨: {JOURNAL_LIST_FILE}")
    
    print(f"\n[INFO] ğŸ”„ ç¬¬äºŒæ­¥ï¼šåŠ è½½RSSæº...")
    rss_feeds = load_rss_feeds_from_csv(JOURNAL_RSS_FILE)
    additional_feeds = [
        {"url": "https://eos.org/feed", "title": "Eos", "source": "additional", "zone": ""},
        {"url": "https://www.sciencedaily.com/rss/earth_climate/geology.xml", "title": "Science Daily Geology", "source": "additional", "zone": ""},
        {"url": "https://news.agu.org/feed/", "title": "AGU News", "source": "additional", "zone": ""},
        {"url": "https://phys.org/rss-feed/earth-news/", "title": "Phys.org Earth News", "source": "additional", "zone": ""},
        {"url": "https://export.arxiv.org/rss/physics.geo-ph", "title": "arXiv Geophysics", "source": "additional", "zone": ""},
        {"url": "http://news.sciencenet.cn/rss/Earth.xml", "title": "ç§‘å­¦ç½‘åœ°çƒç§‘å­¦", "source": "additional", "zone": ""}
    ]
    rss_feeds.extend(additional_feeds)
    
    zone_counts = {}
    for feed in rss_feeds:
        zone = feed.get('zone', 'å…¶ä»–')
        zone_counts[zone] = zone_counts.get(zone, 0) + 1
    
    print(f"[INFO] ğŸ“Š æ€»å…±åŠ è½½RSSæº: {len(rss_feeds)} ä¸ª")
    print(f"[INFO] ğŸ“Š åˆ†åŒºåˆ†å¸ƒ: {', '.join([f'{z}({c}ä¸ª)' for z, c in sorted(zone_counts.items())])}")
    
    print(f"\n[INFO] ğŸ”„ ç¬¬ä¸‰æ­¥ï¼šå¤„ç†æ–‡ç« ...")
    pushed_articles = load_pushed_articles()
    push_schedule = load_push_schedule()
    rss_status = load_rss_status()
    clean_old_records(pushed_articles)
    
    if today not in push_schedule:
        push_schedule[today] = []
    
    all_articles = []
    all_meaningful_phrases = []
    
    def process_timeout_handler(signum, frame):
        raise TimeoutError("å¤„ç†è¶…æ—¶")
    signal.signal(signal.SIGALRM, process_timeout_handler)
    signal.alarm(3600)  # 1å°æ—¶
    
    try:
        print(f"[INFO] ğŸ”„ å¼€å§‹å¤„ç† {len(rss_feeds)} ä¸ªRSSæº...")
        for i, feed_info in enumerate(rss_feeds, 1):
            print(f"[INFO] ğŸ“ˆ å¤„ç†è¿›åº¦: {i}/{len(rss_feeds)}")
            articles, phrases = filter_articles(feed_info, today, pushed_articles, rss_status)
            all_articles.extend(articles)
            all_meaningful_phrases.extend(phrases)
            if i % 5 == 0:
                wait_time = random.uniform(1, 2.5)
                print(f"[INFO] ç­‰å¾… {wait_time:.1f} ç§’...")
                time.sleep(wait_time)
            if i % 20 == 0:
                save_rss_status(rss_status)
                print(f"[INFO] å·²ä¿å­˜å½“å‰RSSçŠ¶æ€ ({i}/{len(rss_feeds)})")
        signal.alarm(0)
    except TimeoutError:
        print("[WARN] RSSæºå¤„ç†è¶…æ—¶ï¼Œä½¿ç”¨å·²å¤„ç†ç»“æœç»§ç»­")
    except Exception as e:
        print(f"[ERROR] RSSæºå¤„ç†å‡ºé”™: {str(e)}")
    finally:
        signal.alarm(0)
        save_rss_status(rss_status)
    
    for article in all_articles:
        if article['hash'] not in [a['hash'] for a in push_schedule[today]]:
            push_schedule[today].append(article)
    
    push_schedule[today] = sorted(push_schedule[today], key=lambda x: x['priority_score'], reverse=True)
    
    if push_schedule[today]:
        print(f"\n[INFO] ğŸ¯ ä¼˜å…ˆçº§æœ€é«˜çš„æ–‡ç« :")
        for i, article in enumerate(push_schedule[today][:5], 1):
            zone_info = f"[{article['zone']}]" if article['zone'] else "[æ— åˆ†åŒº]"
            print(f"  {i}. {article['title'][:60]}... {zone_info} (åˆ†æ•°:{article['priority_score']})")
    
    rss_summary = get_rss_status_summary(rss_status, len(rss_feeds))
    
    print(f"\n[INFO] ğŸ”„ ç¬¬å››æ­¥ï¼šå‡†å¤‡æ¨é€...")
    first_batch = push_schedule[today][:MAX_PUSH_PER_BATCH]
    remaining = push_schedule[today][MAX_PUSH_PER_BATCH:]
    
    if len(first_batch) < MAX_PUSH_PER_BATCH:
        needed_count = MAX_PUSH_PER_BATCH - len(first_batch)
        historical_articles = find_historical_articles(pushed_articles, push_schedule, today, needed_count)
        if historical_articles:
            print(f"[INFO] ğŸ“š ä»å†å²æ–‡ç« è¡¥å……äº† {len(historical_articles)} ç¯‡")
            first_batch.extend(historical_articles)
    
    second_batch = []
    if len(remaining) > 0:
        second_batch = remaining[:MAX_PUSH_PER_BATCH]
        push_schedule[today] = remaining[MAX_PUSH_PER_BATCH:]
    else:
        push_schedule[today] = []
    
    if first_batch:
        print(f"[INFO] âœ… ç¬¬ä¸€æ‰¹æ¬¡æ¨é€ {len(first_batch)} ç¯‡æ–‡ç« ")
        push_content = [format_article_for_push(article, i+1) for i, article in enumerate(first_batch)]
        content = (
            f"ã€ğŸ”ï¸ æŠ˜å åœ°å±‚æ¨é€ã€‘{today} (1/{'2' if second_batch else '1'})\n\n"
            f"{chr(10).join(push_content)}\n\n"
            "ğŸ“Š æ¨é€ç»Ÿè®¡:\n"
            f"ğŸ¯ ç¬¬ä¸€æ‰¹æ¬¡: {len(first_batch)}/{MAX_PUSH_PER_BATCH} ç¯‡\n"
            f"ğŸ” ä»Šæ—¥å‘ç°: {len(all_articles)} ç¯‡æ–°æ–‡ç« \n"
            f"ğŸŒ RSSæˆåŠŸç‡: {rss_summary['success_rate']}% ({rss_summary['success']}/{rss_summary['total']})"
        )
        top_phrases = get_top_meaningful_phrases(all_meaningful_phrases, 5)
        if top_phrases:
            content += "\n\nğŸ”¥ ä»Šæ—¥çƒ­ç‚¹çŸ­è¯­TOP5ï¼š"
            for i, (phrase, count) in enumerate(top_phrases, 1):
                content += f"\nğŸ† {i}. {phrase}: {count}æ¬¡"
        else:
            content += "\n\nğŸ”¥ ä»Šæ—¥çƒ­ç‚¹çŸ­è¯­ï¼š\nğŸš« æš‚æ— æ˜æ˜¾çƒ­ç‚¹çŸ­è¯­"
        content += f"\n\nâ° æ¨é€æ—¶é—´: {current_time}"
        push_to_wechat(content)
        
        if today not in pushed_articles:
            pushed_articles[today] = []
        for article in first_batch:
            pushed_articles[today].append(article['hash'])
        save_pushed_articles(pushed_articles)
        save_push_schedule(push_schedule)
        
        if second_batch:
            print("[INFO] ğŸ•’ ç­‰å¾…5ç§’åæ¨é€ç¬¬äºŒæ‰¹...")
            time.sleep(5)
            current_time = datetime.datetime.now().strftime("%H:%M:%S")
            print(f"[INFO] âœ… ç¬¬äºŒæ‰¹æ¬¡æ¨é€ {len(second_batch)} ç¯‡æ–‡ç« ")
            push_content = [format_article_for_push(article, i+1) for i, article in enumerate(second_batch)]
            content2 = (
                f"ã€ğŸ”ï¸ æŠ˜å åœ°å±‚æ¨é€ã€‘{today} (2/2)\n\n"
                f"{chr(10).join(push_content)}\n\n"
                "ğŸ“Š æ¨é€ç»Ÿè®¡:\n"
                f"ğŸ¯ ç¬¬äºŒæ‰¹æ¬¡: {len(second_batch)}/{MAX_PUSH_PER_BATCH} ç¯‡\n"
                f"ğŸ“‹ é˜Ÿåˆ—å‰©ä½™: {len(push_schedule[today])} ç¯‡\n"
                f"ğŸ” æ€»è®¡å‘ç°: {len(all_articles)} ç¯‡æ–°æ–‡ç« \n\n"
                f"â° æ¨é€æ—¶é—´: {current_time}"
            )
            push_to_wechat(content2)
            for article in second_batch:
                pushed_articles[today].append(article['hash'])
            save_pushed_articles(pushed_articles)
    else:
        print("[INFO] âŒ ä»Šæ—¥æ— æ–°çš„æ ¸å¿ƒå…³é”®è¯åŒ¹é…æ–‡ç« ")
        content = (
            f"ã€ğŸ”ï¸ æŠ˜å åœ°å±‚æ¨é€ã€‘{today}\n\n"
            "ğŸ“ ä»Šæ—¥æ— æ–°çš„æ ¸å¿ƒå…³é”®è¯åŒ¹é…æ–‡ç« \n"
            f"ğŸ” å·²æ£€ç´¢ {rss_summary['total']} ä¸ªRSSæº\n"
            f"âœ… æˆåŠŸè·å– {rss_summary['success']} ä¸ªæº\n"
            f"âŒ å¤±è´¥ {rss_summary['failed']} ä¸ªæº (æˆåŠŸç‡: {rss_summary['success_rate']}%)\n"
            f"ğŸ’­ å…¨åŸŸçŸ­è¯­æå–: {len(all_meaningful_phrases)} ä¸ª"
        )
        top_phrases = get_top_meaningful_phrases(all_meaningful_phrases, 5)
        if top_phrases:
            content += "\n\nğŸ”¥ ä»Šæ—¥çƒ­ç‚¹çŸ­è¯­TOP5ï¼š"
            for i, (phrase, count) in enumerate(top_phrases, 1):
                content += f"\nğŸ† {i}. {phrase}: {count}æ¬¡"
        else:
            content += "\n\nğŸ”¥ ä»Šæ—¥çƒ­ç‚¹çŸ­è¯­ï¼š\nğŸš« æš‚æ— æ˜æ˜¾çƒ­ç‚¹çŸ­è¯­"
        content += f"\n\nâ° æ¨é€æ—¶é—´: {current_time}"
        push_to_wechat(content)
    
    print(f"[INFO] ğŸ‰ æ¨é€å®Œæˆ! ä»Šæ—¥å·²æ¨é€: {len(pushed_articles.get(today, []))}")
    print(f"[INFO] ğŸ“Š RSSæºç»Ÿè®¡: æˆåŠŸ{rss_summary['success']}/å¤±è´¥{rss_summary['failed']}/æ€»è®¡{rss_summary['total']} (æˆåŠŸç‡{rss_summary['success_rate']}%)")
    
    backup_file = f"push_schedule_{today}.json"
    try:
        with open(backup_file, 'w', encoding='utf-8') as f:
            json.dump(push_schedule, f, ensure_ascii=False, indent=2)
        print(f"[INFO] ğŸ“¦ å·²åˆ›å»ºæ¨é€è®¡åˆ’å¤‡ä»½: {backup_file}")
    except Exception as e:
        print(f"[ERROR] å¤‡ä»½æ¨é€è®¡åˆ’å¤±è´¥: {e}")

if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        print(f"[FATAL ERROR] âŒ ä¸»ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
        error_content = (
            "ã€ğŸš¨ æŠ˜å åœ°å±‚æ¨é€ç³»ç»Ÿé”™è¯¯ã€‘\n"
            f"âŒ ç³»ç»Ÿè¿è¡Œå‡ºé”™: {str(e)}\n"
            f"â° é”™è¯¯æ—¶é—´: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n"
            "ğŸ‘¤ ç”¨æˆ·: F-swanlight\n"
            "ğŸ”§ è¯·æ£€æŸ¥ç³»ç»ŸçŠ¶æ€"
        )
        try:
            push_to_wechat(error_content)
        except:
            pass